<?xml version="1.0" encoding="UTF-8"?>
<configuration>

    <appender name="STARTUP"
              class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n
            </pattern>
        </encoder>
    </appender>

    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>log/aero-ebay.log</file>
        <triggeringPolicy
            class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
            <maxFileSize>10MB</maxFileSize>
        </triggeringPolicy>
        <rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy">
            <fileNamePattern>log/aero-ebay%i.log</fileNamePattern>
            <minIndex>1</minIndex>
            <maxIndex>9</maxIndex>
        </rollingPolicy>
        <encoder>
            <pattern>%date %level [%thread] %logger{36} %msg%n</pattern>
            <!-- this improves logging throughput -->
            <immediateFlush>false</immediateFlush>
        </encoder>
    </appender>

    <appender name="KafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <encoder class="com.github.danielwegener.logback.kafka.encoding.LayoutKafkaMessageEncoder">
            <layout class="net.logstash.logback.layout.LogstashLayout" >
                <includeContext>true</includeContext>
                <includeCallerData>true</includeCallerData>
                <customFields>{"component":"event-listener"}</customFields>
                <fieldNames class="net.logstash.logback.fieldnames.ShortenedFieldNames"/>
            </layout>
            <charset>UTF-8</charset>
        </encoder>
        <topic>marketing.tracking.staging.chocolate-log-qa</topic>
        <!-- ensure that every message sent by the executing host is partitioned to the same partition strategy -->
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.HostNameKeyingStrategy" />
        <!-- use async delivery. the application threads are not blocked by logging -->
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy" />
        <!-- even if the producer buffer runs full, do not block the application but start to drop messages -->
        <producerConfig>max.block.ms=120000</producerConfig>
        <producerConfig>request.timeout.ms=3000</producerConfig>
        <producerConfig>acks=0</producerConfig>
        <producerConfig>bootstrap.servers=rhs-nbnaniaa-kfk-lvs-1.rheos-streaming-qa.svc.130.tess.io:9092,rhs-nbnaniaa-kfk-lvs-2.rheos-streaming-qa.svc.130.tess.io:9092,rhs-nbnaniaa-kfk-lvs-3.rheos-streaming-qa.svc.130.tess.io:9092,rhs-nbnaniaa-kfk-lvs-4.rheos-streaming-qa.svc.130.tess.io:9092</producerConfig>
        <producerConfig>sasl.mechanism=IAF</producerConfig>
        <producerConfig>security.protocol=SASL_PLAINTEXT</producerConfig>
        <producerConfig>sasl.jaas.config=io.ebay.rheos.kafka.security.iaf.IAFLoginModule required iafConsumerId="urn:ebay-marketplace-consumerid:0a2563dc-390f-4b78-8648-68c01e248639" iafSecret="f660cc36-bf60-4528-befc-bb2fc203a960" iafEnv="staging";</producerConfig>
        <producerConfig>rheos.services.urls=https://rheos-services.qa.ebay.com</producerConfig>
    </appender>

    <appender name="CAL"
              class="com.ebay.raptor.kernel.logging.CalLoggingAppender">
    </appender>

    <root level="info">
        <appender-ref ref="STARTUP" />
        <appender-ref ref="FILE" />
        <appender-ref ref="KafkaAppender" />
        <appender-ref ref="CAL" />
    </root>

    <logger name="com.ebay.app.raptor.chocolate.eventlistener.util.UnifiedTrackingMessageParser" level="debug" additivity="false">
        <appender-ref ref="KafkaAppender"/>
    </logger>

</configuration>
